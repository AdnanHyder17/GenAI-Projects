# ==== Basic Imports ====
import os
import tempfile  # Used to temporarily save uploaded files during processing
import streamlit as st  # Streamlit for the web interface

# ==== LangChain: Document Loading ====
from langchain_community.document_loaders import PyMuPDFLoader  # Extracts text and metadata from PDF files

# ==== LangChain: Chunking ====
from langchain_text_splitters import RecursiveCharacterTextSplitter  # Breaks long documents into smaller chunks

# ==== LangChain: Vector Embeddings ====
import faiss  # Facebook AI Similarity Search â€” used for fast vector similarity search
from langchain_ollama import OllamaEmbeddings  # Embeddings from Ollama embedding models
from langchain_community.vectorstores import FAISS  # Wraps FAISS index to store and retrieve document vectors
from langchain_community.docstore.in_memory import InMemoryDocstore  # Keeps documents in memory for quick access

# ==== LangChain: RAG (Retrieval-Augmented Generation) ====
from langchain_ollama import ChatOllama  # Chat model from Ollama (like llama3.2)
from langchain_core.prompts import ChatPromptTemplate  # Helps build structured prompts
from langchain_core.output_parsers import StrOutputParser  # Parses LLM responses as plain strings
from langchain_core.runnables import RunnablePassthrough, RunnableLambda  # Used in pipeline logic for RAG chain

# ==== Streamlit: App Initialization ====
# Initialize state variables across user interactions
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "current_question" not in st.session_state:
    st.session_state.current_question = ""
if "current_answer" not in st.session_state:
    st.session_state.current_answer = ""

# ==== App UI Title ====
st.title("Chat with any PDF")
st.sidebar.title("Settings")

# ==== Sidebar: Model and Embedding Options ====
model_option = st.sidebar.selectbox(
    "Select LLM Model", ["llama3.2:1b"], index=0
)
embedding_option = st.sidebar.selectbox(
    "Select Embedding Model", ["nomic-embed-text"], index=0
)

# ==== Sidebar: Clear Chat Button ====
if st.sidebar.button("Clear Chat History"):
    st.session_state.chat_history = []
    st.session_state.current_question = ""
    st.session_state.current_answer = ""

# ==== Function: Load, Process, and Chunk Uploaded Documents ====
def process_documents(uploaded_files):
    all_pages_content = []
    doc_locations = []

    for i, doc in enumerate(uploaded_files):
        # Save uploaded file to a temporary local path
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(doc.read())
            tmp_path = tmp_file.name
            doc_locations.append(tmp_path)

        # Load and extract content from PDF
        loader = PyMuPDFLoader(tmp_path)
        pages = loader.load()

        # Add metadata (source name and page number)
        for page in pages:
            page.metadata["source"] = doc.name
            page.metadata["page"] = page.metadata.get("page", 0) + 1

        all_pages_content.extend(pages)

    # Chunk large texts into manageable parts for embedding
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = text_splitter.split_documents(all_pages_content)

    return chunks, doc_locations

# ==== Function: Create a Vector Store from Chunks ====
def create_vector_store(chunks, embedding_option):
    with st.spinner("Creating vector store..."):
        # Generate vector embeddings for text chunks
        embeddings = OllamaEmbeddings(model=embedding_option, base_url='http://localhost:11434')
        vector_dimension = len(embeddings.embed_query("hello world"))

        # Create FAISS index with those embeddings
        index = faiss.IndexFlatL2(vector_dimension)
        vector_store = FAISS(
            embedding_function=embeddings,
            index=index,
            docstore=InMemoryDocstore(),  # Stores the actual documents
            index_to_docstore_id={},  # Mapping from index to document ID
        )

        # Add document chunks to the FAISS index
        vector_store.add_documents(documents=chunks)
        return vector_store

# ==== Streamlit: File Upload Widget ====
uploaded_file = st.file_uploader("Upload PDFs", type="pdf", accept_multiple_files=True)

# ==== Trigger: Process Uploaded Files ====
if uploaded_file:
    if st.button("Process Documents"):
        chunks, doc_locations = process_documents(uploaded_file)
        st.session_state.vector_store = create_vector_store(chunks, embedding_option)

        st.success(f"Processed {len(uploaded_file)} documents with {len(chunks)} total chunks")

        # Delete temporary files from disk
        for doc in doc_locations:
            try:
                os.remove(doc)
            except:
                pass

# ==== Function: Handle Q&A Interaction ====
def handle_question():
    question = st.session_state.question_input
    st.session_state.question_input = ""  # Reset input box

    if question and st.session_state.vector_store:
        # Store previous question/answer in history
        if st.session_state.current_question and st.session_state.current_answer:
            st.session_state.chat_history.append({
                "question": st.session_state.current_question,
                "answer": st.session_state.current_answer
            })

        st.session_state.current_question = question

        with st.spinner("Searching documents and generating answer..."):
            # Retrieve most relevant chunks based on similarity
            retrieved_chunks = st.session_state.vector_store.similarity_search(query=question, k=3)

            # Combine retrieved content into a single context string
            context = "\n\n".join([doc.page_content for doc in retrieved_chunks])

            # Create LLM (Ollama backend)
            model = ChatOllama(model=model_option, base_url='http://localhost:11434', temperature=1.2)

            # Define prompt template
            prompt = """
            You are a helpful assistant. Use the following extracted content from documents to answer the user's question.
            If the answer is not found in the content, respond with: "This information is not present in the uploaded documents."

            Answer in exactly three bullet points, with a blank line between each bullet.

            Question: {question}

            Context:
            {context}

            Answer:
            """

            # Build RAG chain using the prompt and model
            prompt_template = ChatPromptTemplate.from_template(prompt)
            rag_chain = (
                {"context": RunnableLambda(lambda _: context), "question": RunnablePassthrough()}
                | prompt_template
                | model
                | StrOutputParser()
            )

            # Invoke model to get the answer
            response = rag_chain.invoke(question)
            st.session_state.current_answer = response

            # Show sources used in the response
            with st.expander("View Source Documents"):
                for doc in retrieved_chunks:
                    source = doc.metadata.get("source", "Unknown")
                    page = doc.metadata.get("page", "Unknown")
                    st.markdown(f"**{source} (Page {page})**")
                    st.markdown(doc.page_content)
                    st.divider()

# ==== UI: Input and Display Section ====
st.subheader("Ask questions about your documents")

# Input box for user's question
st.text_input("Enter your question:", key="question_input", on_change=handle_question)

# Display current Q&A
if st.session_state.current_question and st.session_state.current_answer:
    st.subheader(f"Question: {st.session_state.current_question}")
    st.markdown(st.session_state.current_answer)

# Display past chat history
if len(st.session_state.chat_history) > 0:
    st.subheader("Chat History")
    for message in st.session_state.chat_history:
        st.markdown(f"**Question:** {message['question']}")
        st.markdown(f"**Answer:** {message['answer']}")
        st.divider()

# Instructions when PDFs aren't loaded or processed yet
if not st.session_state.vector_store and not uploaded_file:
    st.info("Upload PDF documents to start chatting with them.")
elif not st.session_state.vector_store and uploaded_file:
    st.info("Click 'Process Documents' to analyze the uploaded PDFs.")